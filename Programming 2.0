# Import necessary packages
import os
import pandas as pd
import numpy as np
import cryptocode
import requests

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams["figure.figsize"] = (16,6)
plotsize = (16, 5)

from pyspark.dbutils import DBUtils
from pyspark.sql import SparkSession
from datetime import datetime, date, time, timedelta
from databricks.sdk.runtime import *
dbutils.fs.cp("dbfs:/FileStore/Snowflake_Databricks_Utility.py", "file:/tmp/Snowflake_Databricks_Utility.py", True)
import sys
sys.path.append("/tmp")
from Snowflake_Databricks_Utility import SnowFlakeUtility
import warnings
pd.set_option("display.max_columns", None)
warnings.filterwarnings("ignore")

# Replace the USER_ROLE(Role name should be in CAPS) with the role you have access to.
myinstance = SnowFlakeUtility(user_role="HR_REAL_ESTATE")
myinstance.execute()

sfOptions = myinstance.sfOptions

# Fetch Office Data from server
office_query = '''
SELECT
  DISTINCT facility_code,
  TTL_SEATS AS NUMBER_OF_SEATS
FROM
  HRDW.REAL_ESTATE.real_estate_build
WHERE
  DATE_TRUNC('MONTH', CURRENT_DATE) - INTERVAL '1 MONTH' = DATE_TRUNC('DAY', calendar_date) AND
  status_description = 'Active' AND
  FACILITY_CODE = '014I'
'''

print('thinking...')
office_spdf = spark.read.format("snowflake") \
    .options(**sfOptions) \
    .option("query", office_query) \
    .option("sfWarehouse", "BIZ_REPORTING_WH") \
    .load()

office_df = office_spdf.toPandas()
print('Shape of Office Data is', office_df.shape)

# Fetch Date Data from server
date_query = '''
SELECT
  calendar_date,
  day_name
FROM
  HRDW.EMPLOYEE_ORG.CALENDAR
WHERE
 YEAR_ACTUAL >= '2023'
'''

print('thinking...')
date_spdf = spark.read.format("snowflake") \
    .options(**sfOptions) \
    .option("query", date_query) \
    .option("sfWarehouse", "BIZ_REPORTING_WH") \
    .load()

date_df = date_spdf.toPandas()
print('Shape of Date Data is', date_df.shape)

# Fetch Badge Data from server
badge_query = '''
SELECT
  date,
  employee_number,
  office_number
FROM
  HRDW.REAL_ESTATE.badge_fact
WHERE
  date >= '2023-09-05' AND
  office_number = '014I'
'''

print('thinking...')
badge_spdf = spark.read.format("snowflake") \
    .options(**sfOptions) \
    .option("query", badge_query) \
    .option("sfWarehouse", "BIZ_REPORTING_WH") \
    .load()

badge_df = badge_spdf.toPandas()
print('Shape of Badge Data is', badge_df.shape)

# Create the count_for_swipes column
badge_df['count_for_swipes'] = badge_df.apply(lambda row: f"{row['DATE']}_{row['OFFICE_NUMBER']}_{row['EMPLOYEE_NUMBER']}", axis=1)

# Group by DATE and OFFICE_NUMBER and count distinct occurrences of count_for_swipes
daily_swipe_counts = badge_df.groupby(['DATE', 'OFFICE_NUMBER'])['count_for_swipes'].nunique().reset_index(name='distinct_swipe_count')

# Merge daily_swipe_counts with office_df on OFFICE_NUMBER and FACILITY_CODE
merged_df = daily_swipe_counts.merge(
    office_df[['FACILITY_CODE', 'NUMBER_OF_SEATS']],  # Select only the columns needed from office_df
    left_on='OFFICE_NUMBER',
    right_on='FACILITY_CODE',
    how='left'  # Use 'left' to keep all records from daily_swipe_counts
)

# Select only the required columns
final_df = merged_df[['DATE', 'OFFICE_NUMBER', 'distinct_swipe_count', 'NUMBER_OF_SEATS']]

# Ensure the actual column name is correctly referenced
final_df.columns = final_df.columns.str.upper()  # If unsure, convert all to uppercase to align with CALENDAR_DATE

# Ensure both DATE columns are of the datetime type for accurate merging
final_df['DATE'] = pd.to_datetime(final_df['DATE'])  # Confirm 'DATE' is correct in final_df
date_df['CALENDAR_DATE'] = pd.to_datetime(date_df['CALENDAR_DATE'])

# Merge final_df with date_df to include the DAY_NAME
final_df_with_day_name = final_df.merge(
    date_df[['CALENDAR_DATE', 'DAY_NAME']],
    left_on='DATE',
    right_on='CALENDAR_DATE',
    how='left'
)

# Drop the redundant CALENDAR_DATE column
final_df_with_day_name.drop(columns=['CALENDAR_DATE'], inplace=True)

# Strip whitespace from the 'DAY_NAME' column
final_df_with_day_name['DAY_NAME'] = final_df_with_day_name['DAY_NAME'].str.strip()

# Filter out weekends and define weekdays_df
weekdays_df = final_df_with_day_name[~final_df_with_day_name['DAY_NAME'].isin(['Saturday', 'Sunday'])]

# Check if weekdays_df is correctly defined
print("Unique days in weekdays_df after filtering:")
print(weekdays_df['DAY_NAME'].unique())  # Should only show ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']

def analyze_office_space_utilization(final_df_with_day_name):
    """
    Comprehensive analysis of office space utilization
    
    Args:
        final_df_with_day_name (pd.DataFrame): DataFrame with occupancy data
    
    Returns:
        dict: Comprehensive analysis results
    """
    # Ensure numeric types
    weekdays_df['DISTINCT_SWIPE_COUNT'] = pd.to_numeric(weekdays_df['DISTINCT_SWIPE_COUNT'], errors='coerce')
    weekdays_df['NUMBER_OF_SEATS'] = pd.to_numeric(weekdays_df['NUMBER_OF_SEATS'], errors='coerce')
    
    # Analysis Metrics
    results = {
        'total_days_tracked': 0,
        'average_daily_occupancy': 0,
        'average_seat_utilization': 0,
        'optimal_seat_count': 0,
        'potential_seat_reduction': 0,
        'overcapacity_days_count': 0,
        'overcapacity_percentage': 0,
        'peak_usage': 0,
        'peak_usage_percentage': 0,
        'usage_standard_deviation': 0,
        'coefficient_of_variation': 0
    }
    
    # 1. Overall Occupancy Analysis
    results['total_days_tracked'] = len(weekdays_df)
    results['average_daily_occupancy'] = weekdays_df['DISTINCT_SWIPE_COUNT'].mean()
    
    # Safely handle seat count
    try:
        total_seats = weekdays_df['NUMBER_OF_SEATS'].iloc[0]
        if pd.notna(total_seats) and total_seats != 0:
            results['average_seat_utilization'] = (results['average_daily_occupancy'] / total_seats) * 100
            
            # 3. Capacity Planning
            target_utilization = 0.80  # 80% target
            results['optimal_seat_count'] = int(results['average_daily_occupancy'] / target_utilization) if results['average_daily_occupancy'] > 0 else 0
            results['potential_seat_reduction'] = max(0, total_seats - results['optimal_seat_count'])
            
            # 4. Overcapacity Analysis
            overcapacity_days = weekdays_df[weekdays_df['DISTINCT_SWIPE_COUNT'] > total_seats]
            results['overcapacity_days_count'] = len(overcapacity_days)
            results['overcapacity_percentage'] = len(overcapacity_days) / len(weekdays_df) * 100 if len(weekdays_df) > 0 else 0
            
            # 5. Peak Usage Analysis
            results['peak_usage'] = weekdays_df['DISTINCT_SWIPE_COUNT'].max()
            results['peak_usage_percentage'] = results['peak_usage'] / total_seats * 100
            
            # 6. Variability in Usage
            results['usage_standard_deviation'] = weekdays_df['DISTINCT_SWIPE_COUNT'].std()
            results['coefficient_of_variation'] = results['usage_standard_deviation'] / results['average_daily_occupancy'] * 100 if results['average_daily_occupancy'] > 0 else 0
    except Exception as e:
        print(f"Error in seat count analysis: {e}")
    
    # 2. Day of Week Analysis
    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    day_of_week_analysis = weekdays_df.groupby('DAY_NAME')['DISTINCT_SWIPE_COUNT'].agg(['mean', 'count']).reset_index()
    
    # Ensure all weekdays are represented, even if they have no data
    day_df = pd.DataFrame({'DAY_NAME': day_order})
    day_of_week_analysis = day_df.merge(day_of_week_analysis, on='DAY_NAME', how='left').fillna(0)
    
    # Safely calculate utilization percentage
    if pd.notna(total_seats) and total_seats != 0:
        day_of_week_analysis['utilization_percentage'] = day_of_week_analysis['mean'] / total_seats * 100
    else:
        day_of_week_analysis['utilization_percentage'] = 0
    
    results['day_of_week_analysis'] = day_of_week_analysis
    
    return results

def visualize_office_utilization(final_df_with_day_name, results):
    """
    Create visualizations for office space utilization
    
    Args:
        final_df_with_day_name (pd.DataFrame): DataFrame with occupancy data
        results (dict): Analysis results from analyze_office_space_utilization
    """
    # Filter out weekends
    weekdays_df = final_df_with_day_name[~final_df_with_day_name['DAY_NAME'].isin(['Saturday', 'Sunday'])]
    
    plt.figure(figsize=(15, 10))
    
    # Day of Week Utilization
    plt.subplot(2, 2, 1)
    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']
    day_analysis = results['day_of_week_analysis'].set_index('DAY_NAME')
    day_analysis = day_analysis.reindex(day_order)
    
    day_analysis['utilization_percentage'].plot(kind='bar')
    plt.title('Daily Utilization Percentage')
    plt.xlabel('Day of Week')
    plt.ylabel('Utilization %')
    plt.xticks(rotation=45)
    
    # Daily Occupancy Distribution
    plt.subplot(2, 2, 2)
    weekdays_df['DISTINCT_SWIPE_COUNT'].hist(bins=20)
    plt.title('Distribution of Daily Occupancy')
    plt.xlabel('Number of Occupants')
    plt.ylabel('Frequency')
    
    # Box Plot of Occupancy by Day
    plt.subplot(2, 2, 3)
    sns.boxplot(x='DAY_NAME', y='DISTINCT_SWIPE_COUNT', 
                data=weekdays_df, 
                order=day_order)
    plt.title('Occupancy Distribution by Day')
    plt.xlabel('Day of Week')
    plt.ylabel('Number of Occupants')
    plt.xticks(rotation=45)
    
    # Seat Capacity vs Actual Usage
    plt.subplot(2, 2, 4)
    plt.scatter(weekdays_df['NUMBER_OF_SEATS'], weekdays_df['DISTINCT_SWIPE_COUNT'])
    plt.title('Seat Capacity vs Actual Usage')
    plt.xlabel('Total Seats')
    plt.ylabel('Actual Occupancy')
    
    plt.tight_layout()
    plt.show()

def generate_space_optimization_report(results):
    """
    Generate a detailed report on space optimization opportunities
    
    Args:
        results (dict): Analysis results from analyze_office_space_utilization
    
    Returns:
        str: Detailed markdown report
    """
    report = f"""# Office Space Utilization Report

## Key Metrics
- **Total Days Tracked**: {results['total_days_tracked']}
- **Average Daily Occupancy**: {results['average_daily_occupancy']:.2f}
- **Average Seat Utilization**: {results['average_seat_utilization']:.2f}%

## Capacity Planning
- **Current Total Seats**: {results['optimal_seat_count']}
- **Potential Seat Reduction**: {results['potential_seat_reduction']}
- **Recommended Optimal Seat Count**: {results['optimal_seat_count']}

## Overcapacity Analysis
- **Overcapacity Days**: {results['overcapacity_days_count']}
- **Overcapacity Percentage**: {results['overcapacity_percentage']:.2f}%

## Usage Variability
- **Peak Usage**: {results['peak_usage']}
- **Peak Usage Percentage**: {results['peak_usage_percentage']:.2f}%
- **Usage Standard Deviation**: {results['usage_standard_deviation']:.2f}
- **Coefficient of Variation**: {results['coefficient_of_variation']:.2f}%

## Recommendations
1. Consider reducing total seats by {results['potential_seat_reduction']} to optimize space utilization
2. Focus on days with lower utilization for potential space reallocation
3. Investigate reasons for overcapacity on {results['overcapacity_days_count']} days
"""
    return report

# Run the analysis
results = analyze_office_space_utilization(final_df_with_day_name)

# Generate visualizations
visualize_office_utilization(final_df_with_day_name, results)

# Generate and print the report
report = generate_space_optimization_report(results)
print(report)



----------


def perform_time_series_decomposition(final_df_with_day_name):
    """
    Perform comprehensive time series decomposition of office occupancy data
    
    Args:
        final_df_with_day_name (pd.DataFrame): DataFrame with daily occupancy data
    
    Returns:
        dict: Decomposition results and visualizations
    """
    # Prepare time series data
    daily_occupancy = final_df_with_day_name.groupby('DATE')['DISTINCT_SWIPE_COUNT'].mean().reset_index()
    daily_occupancy.set_index('DATE', inplace=True)
    
    # Ensure time series is sorted
    daily_occupancy.sort_index(inplace=True)
    
    # Perform time series decomposition using seasonal_decompose
    from statsmodels.tsa.seasonal import seasonal_decompose
    
    # Decomposition results
    decomposition_results = {}
    
    try:
        # Decompose with period set to 5 for weekly seasonality (typical work week)
        decomposition = seasonal_decompose(daily_occupancy['DISTINCT_SWIPE_COUNT'], period=5)
        
        # Store decomposition components
        decomposition_results = {
            'original': decomposition.observed,
            'trend': decomposition.trend,
            'seasonal': decomposition.seasonal,
            'residual': decomposition.resid
        }
        
        # Visualization of decomposition
        plt.figure(figsize=(15, 10))
        
        # Original Data
        plt.subplot(5, 1, 1)
        plt.plot(daily_occupancy.index, decomposition_results['original'])
        plt.title('Original Time Series')
        
        # Trend Component
        plt.subplot(5, 1, 2)
        plt.plot(daily_occupancy.index, decomposition_results['trend'])
        plt.title('Trend Component')
        
        # Seasonal Component
        plt.subplot(5, 1, 3)
        plt.plot(daily_occupancy.index, decomposition_results['seasonal'])
        plt.title('Seasonal Component')
        
        # Residual Component
        plt.subplot(5, 1, 4)
        plt.plot(daily_occupancy.index, decomposition_results['residual'])
        plt.title('Residual Component')
        
        # Histogram of Residuals
        plt.subplot(5, 1, 5)
        plt.hist(decomposition_results['residual'].dropna(), bins=30)
        plt.title('Residual Distribution')
        
        plt.tight_layout()
        plt.show()
        
        # Statistical analysis of decomposition
        decomposition_stats = {
            'trend_analysis': {
                'mean': decomposition_results['trend'].mean(),
                'standard_deviation': decomposition_results['trend'].std(),
                'long_term_change': decomposition_results['trend'].iloc[-1] - decomposition_results['trend'].iloc[0]
            },
            'seasonal_analysis': {
                'peak_seasonal_impact': decomposition_results['seasonal'].abs().max(),
                'mean_seasonal_variation': decomposition_results['seasonal'].abs().mean()
            },
            'residual_analysis': {
                'mean': decomposition_results['residual'].mean(),
                'standard_deviation': decomposition_results['residual'].std(),
                'is_white_noise': False  # Placeholder for statistical white noise test
            }
        }
        
        # Perform white noise test on residuals
        from scipy import stats
        _, p_value = stats.normaltest(decomposition_results['residual'].dropna())
        decomposition_stats['residual_analysis']['is_white_noise'] = p_value > 0.05
        
        return {
            'decomposition_results': decomposition_results,
            'decomposition_stats': decomposition_stats
        }
    
    except Exception as e:
        print(f"Error in time series decomposition: {e}")
        return None

# Add this to your main script after the initial analysis
ts_decomposition = perform_time_series_decomposition(final_df_with_day_name)

# Print out key insights if decomposition was successful
if ts_decomposition:
    print("\n--- Time Series Decomposition Insights ---")
    print("\nTrend Analysis:")
    for key, value in ts_decomposition['decomposition_stats']['trend_analysis'].items():
        print(f"{key}: {value}")
    
    print("\nSeasonal Analysis:")
    for key, value in ts_decomposition['decomposition_stats']['seasonal_analysis'].items():
        print(f"{key}: {value}")
    
    print("\nResidual Analysis:")
    for key, value in ts_decomposition['decomposition_stats']['residual_analysis'].items():
        print(f"{key}: {value}")

--------------


def prepare_time_series_data(final_df_with_day_name):
    """
    Prepare time series data for predictive modeling
    
    Args:
        final_df_with_day_name (pd.DataFrame): DataFrame with daily occupancy data
    
    Returns:
        pd.DataFrame: Prepared time series dataframe
    """
    # Group by date and calculate mean daily occupancy
    daily_occupancy = final_df_with_day_name.groupby('DATE')['DISTINCT_SWIPE_COUNT'].mean().reset_index()
    daily_occupancy.set_index('DATE', inplace=True)
    daily_occupancy.sort_index(inplace=True)
    
    # Create lag features and rolling statistics
    daily_occupancy['lag_1'] = daily_occupancy['DISTINCT_SWIPE_COUNT'].shift(1)
    daily_occupancy['lag_7'] = daily_occupancy['DISTINCT_SWIPE_COUNT'].shift(7)
    daily_occupancy['rolling_mean_7'] = daily_occupancy['DISTINCT_SWIPE_COUNT'].rolling(window=7).mean()
    daily_occupancy['rolling_std_7'] = daily_occupancy['DISTINCT_SWIPE_COUNT'].rolling(window=7).std()
    
    # Add cyclical time features
    daily_occupancy['day_of_week'] = daily_occupancy.index.dayofweek
    daily_occupancy['month'] = daily_occupancy.index.month
    
    # Drop NaN rows created by lag and rolling features
    daily_occupancy.dropna(inplace=True)
    
    return daily_occupancy

def predictive_occupancy_modeling(final_df_with_day_name):
    """
    Implement multiple predictive modeling techniques for office occupancy
    
    Args:
        final_df_with_day_name (pd.DataFrame): DataFrame with daily occupancy data
    
    Returns:
        dict: Predictive modeling results and evaluations
    """
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.linear_model import LinearRegression
    from prophet import Prophet
    from statsmodels.tsa.arima.model import ARIMA
    
    # Prepare time series data
    daily_occupancy = prepare_time_series_data(final_df_with_day_name)
    
    # Prepare features and target
    X = daily_occupancy.drop('DISTINCT_SWIPE_COUNT', axis=1)
    y = daily_occupancy['DISTINCT_SWIPE_COUNT']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Predictive Models
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
    }
    
    # Model evaluation results
    model_results = {}
    
    # Train and evaluate machine learning models
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        
        model_results[name] = {
            'MAE': mean_absolute_error(y_test, y_pred),
            'MSE': mean_squared_error(y_test, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
            'R2': r2_score(y_test, y_pred)
        }
    
    # Prophet Forecasting
    prophet_df = daily_occupancy.reset_index().rename(columns={'DATE': 'ds', 'DISTINCT_SWIPE_COUNT': 'y'})
    prophet_model = Prophet(
        changepoint_prior_scale=0.05,
        seasonality_prior_scale=10,
        holidays_prior_scale=10
    )
    prophet_model.add_seasonality(name='weekly', period=7, fourier_order=3)
    prophet_model.fit(prophet_df)
    
    # Prophet Future Forecast
    future = prophet_model.make_future_dataframe(periods=30)  # 30 days forecast
    forecast = prophet_model.predict(future)
    
    # ARIMA Modeling
    arima_model = ARIMA(y, order=(5,1,2))
    arima_fit = arima_model.fit()
    
    # Forecast with Confidence Intervals
    forecast_horizon = 30
    arima_forecast = arima_fit.get_forecast(steps=forecast_horizon)
    arima_pred = arima_forecast.predicted_mean
    arima_ci = arima_forecast.conf_int()
    
    # Ensemble Method (Simple Average)
    ensemble_predictions = {}
    for name, model in models.items():
        ensemble_predictions[name] = model.predict(X_test_scaled)
    
    ensemble_pred = np.mean(list(ensemble_predictions.values()), axis=0)
    
    # Overcapacity Likelihood
    seats_total = daily_occupancy['NUMBER_OF_SEATS'].iloc[0]  # Assumes consistent seat count
    overcapacity_threshold = seats_total * 0.9  # 90% utilization threshold
    
    model_results['Overcapacity Analysis'] = {
        'Overcapacity Probability': np.mean(y_pred > overcapacity_threshold),
        'Average Predicted Occupancy': np.mean(y_pred),
        'Max Predicted Occupancy': np.max(y_pred)
    }
    
    # Visualize Forecasts
    plt.figure(figsize=(15, 10))
    
    # Actual vs Predicted
    plt.subplot(2, 2, 1)
    plt.plot(y_test.index, y_test.values, label='Actual')
    plt.plot(y_test.index, y_pred, label='Predicted', alpha=0.7)
    plt.title('Actual vs Predicted Occupancy')
    plt.legend()
    
    # Prophet Forecast
    plt.subplot(2, 2, 2)
    plt.plot(forecast['ds'], forecast['yhat'], label='Forecast')
    plt.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], alpha=0.3)
    plt.title('Prophet Forecast with Uncertainty')
    
    # ARIMA Forecast
    plt.subplot(2, 2, 3)
    plt.plot(arima_pred.index, arima_pred.values, label='ARIMA Forecast')
    plt.fill_between(arima_ci.index, 
                     arima_ci.iloc[:, 0], 
                     arima_ci.iloc[:, 1], 
                     alpha=0.3)
    plt.title('ARIMA Forecast with Confidence Interval')
    
    # Ensemble Predictions
    plt.subplot(2, 2, 4)
    plt.plot(y_test.index, ensemble_pred, label='Ensemble Prediction')
    plt.title('Ensemble Model Predictions')
    
    plt.tight_layout()
    plt.show()
    
    return {
        'model_performance': model_results,
        'prophet_forecast': forecast,
        'arima_forecast': {
            'predictions': arima_pred,
            'confidence_intervals': arima_ci
        }
    }

# Run the predictive modeling
predictive_results = predictive_occupancy_modeling(final_df_with_day_name)

# Print out key insights
print("\n--- Predictive Modeling Insights ---")
print("\nModel Performance:")
for model, metrics in predictive_results['model_performance'].items():
    print(f"\n{model}:")
    for metric, value in metrics.items():
        print(f"{metric}: {value}")

# Optional: Detailed forecast analysis can be added here
